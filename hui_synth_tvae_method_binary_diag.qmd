---
title: "Analysis"
date: "`r Sys.Date()`"
author: "Rich Evans, PhD, PSTAT"
format: pdf
number-sections: true
execute: 
  echo: true
output-file: "analysis_results"
reticulate:
  python: "/Users/richardevans/.virtualenvs/r-reticulate/bin/python"
---

Got it! Hereâ€™s a compact, ready-to-run R script that:
	1.	Generates a practice dataset: first column is a binary class (0/1), second and third are continuous diagnostic tests, each correlated with the class (and moderately with each other).
	2.	Uses reticulate to call Pythonâ€™s SDV TVAE and create a synthetic dataset with similar structure.

Paste this whole script into R. It will install missing Python deps if needed, fit TVAE, and return a synthetic data.frame.



```{r}
# Restart R first!
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")

library(reticulate)
py_config()  # should show ~/.virtualenvs/r-reticulate/bin/python


py_run_string("
import sys, subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])
")


py_install('torch', pip = TRUE)

py_install(c('sdv==1.12.1', 'rdt>=1.10.0', 'copulas>=0.11.0'), pip = TRUE)

pd   <- import("pandas")
sdvT <- import("sdv.tabular")   # <- this is where it previously failed
TVAE <- sdvT$TVAE
pd$`__version__`

```


```{r}
#| label: setup
#| echo: false
#| include: false

# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}

# Set global chunk options
knitr::opts_chunk$set(
  echo = FALSE,         # Show code by default
  warning = FALSE,     # Hide warnings in the output
  message = FALSE,     # Hide messages in the output
  fig.width = 4,       # Default figure width
  fig.height = 2.666,      # Default figure height
  fig.align = "center" # Center align figures
)

# Set seed for reproducibility
#set.seed(1234)

```


Greatâ€”hereâ€™s a drop-in rewrite where both diagnostic tests are binary (0/1) and the synthetic data are also binary.
Key tweaks vs. your previous script:
	â€¢	The practice data now simulate test1/test2 as Bernoulli variables with class-specific rates.
	â€¢	We explicitly tell SDV (via metadata) that class, test1, and test2 are boolean so it learns/generates 0/1.
	â€¢	Conditional sampling controls the synthetic class proportion (not fixed to the original).



This is the practice "real" data. It is three binary columns, and the first is the class label. 
```{r}

# --- 0) Setup (restart R first if you switched Python) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")


# --- 1) Practice binary data (0/1) ---
#set.seed(123)
n <- 1000
class <- c(rep(0,250),rep(1,750)) # prevalence 75%

p1_0 <- 0.15 # 1 - sp
p1_1 <- 0.75 #se

test1 <- ifelse(class==1, rbinom(n,1,p1_1), rbinom(n,1,p1_0))

p2_0 <- 0.10 # 1 -sp test 2
p2_1 <- 0.65 # se test 2

test2 <- ifelse(class==1, rbinom(n,1,p2_1), rbinom(n,1,p2_0))

df <- data.frame(
  class = as.integer(class),
  test1 = as.integer(test1),
  test2 = as.integer(test2)
)

```



verify se and sp in df. Note that above the input is 1-sp

```{r}
# SE and SP calculator

# df has columns: class, test1, test2

calc_se_sp <- function(df) {
  gold <- df$class
  
  se_sp <- function(test) {
    TP <- sum(test == 1 & gold == 1)
    FN <- sum(test == 0 & gold == 1)
    TN <- sum(test == 0 & gold == 0)
    FP <- sum(test == 1 & gold == 0)
    c(Sensitivity = TP / (TP + FN),
      Specificity = TN / (TN + FP))
  }
  
  results <- sapply(df[, c("test1", "test2")], se_sp)
  round(results, 3)
}

# Example:
 calc_se_sp(df)


```

This takes the practice "real" data and changes it into another similar dataset

```{r}


library(reticulate)

pd          <- import("pandas")
sdvS        <- import("sdv.single_table")
sdvM        <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")



# --- 2) Metadata: declare 0/1 as categorical (robust across versions) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
metadata$add_column(column_name = "class", sdtype = "categorical")
metadata$add_column(column_name = "test1", sdtype = "categorical")
metadata$add_column(column_name = "test2", sdtype = "categorical")

py_df <- r_to_py(df)

TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)

# (Optional) Validate training data against metadata & do a tiny smoke sample
metadata$validate_data(py_df)
invisible(tvae$sample(as.integer(5L)))

# --- 3) Target synthetic size & class mix ---
n_synth  <- 1000      # total desired synthetic samples
p_class1 <- 0.30      # target proportion for class = 1
p_class1 <- max(0, min(1, p_class1))  # clamp to [0,1] just in case


n1 <- round(n_synth * p_class1)   # number of class 1 samples
n0 <- n_synth - n1                # number of class 0 samples

#create counts for each class if there aren't any as a safeguard
if (p_class1 > 0 && n1 == 0) n1 <- 1
if (p_class1 < 1 && n0 == 0) n0 <- 1

#Guarantees that if a class should exist (probability > 0), at least one record of that class is generated.
if (n0 + n1 != n_synth) {
  if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
}

# --- 4) Conditional sampling with Condition blocks (no DataFrameCondition) ---
#	â€¢	sdvSampling$Condition is a Python class (from the SDV Synthetic Data Vault library).
#	â€¢	Each Condition tells the TVAE sampler:
#	â€¢	how many rows to generate (num_rows)
#	â€¢	which column(s) should take fixed values (column_values)

#Here, youâ€™re telling it:
#	â€¢	make n0 samples where class = 0
#	â€¢	make n1 samples where class = 1

Condition <- sdvSampling$Condition
conds <- list(
  Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
  Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
)

#Sampling from the trained model
	#â€¢	The result (py_syn) is a Python object converted to an R data frame with py_to_r().
py_syn <- tvae$sample_from_conditions(conds)
synthetic_df <- py_to_r(py_syn)

# Shuffle to avoid two big class blocks (optional)
# set.seed(42)
# synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), ]
# row.names(synthetic_df) <- NULL

# Ensure column order and 0/1 integers
synthetic_df <- within(synthetic_df[, c("class","test1","test2")], {
  class <- as.integer(class); test1 <- as.integer(test1); test2 <- as.integer(test2)
})

# --- 5) Quick checks ---
cat("\nTarget class1 proportion:", p_class1, "\n")
print(prop.table(table(synthetic_df$class)))

list(calc_se_sp(df),calc_se_sp(synthetic_df))

```




```{r}



# Huiâ€“Walter MLE (2 tests Ã— 2 populations), with SEs and Wald CIs
# pop1, pop2: 2x2 integer matrices (rows T1=0/1, cols T2=0/1)


hui_walter_mle_se <- function(pop1, pop2, conf_level = 0.95) {
  stopifnot(is.matrix(pop1), is.matrix(pop2),
            all(dim(pop1) == c(2,2)), all(dim(pop2) == c(2,2)))
  invlogit <- function(eta) 1/(1+exp(-eta))
  logit    <- function(p) log(p/(1-p))

  # Likelihood pieces
  Pcells <- function(se1, sp1, se2, sp2, pi) {
    c(
      p00 = pi*(1-se1)*(1-se2) + (1-pi)*sp1*sp2,
      p01 = pi*(1-se1)*se2     + (1-pi)*sp1*(1-sp2),
      p10 = pi*se1*(1-se2)     + (1-pi)*(1-sp1)*sp2,
      p11 = pi*se1*se2         + (1-pi)*(1-sp1)*(1-sp2)
    )
  }
  nll <- function(par) {
    se1 <- invlogit(par[1]); sp1 <- invlogit(par[2])
    se2 <- invlogit(par[3]); sp2 <- invlogit(par[4])
    pi1 <- invlogit(par[5]); pi2 <- invlogit(par[6])

    p1 <- pmax(Pcells(se1, sp1, se2, sp2, pi1), 1e-12)
    p2 <- pmax(Pcells(se1, sp1, se2, sp2, pi2), 1e-12)

    # counts in (00,01,10,11) order
    x1 <- c(pop1[1,1], pop1[1,2], pop1[2,1], pop1[2,2])
    x2 <- c(pop2[1,1], pop2[1,2], pop2[2,1], pop2[2,2])

    -(sum(x1*log(p1)) + sum(x2*log(p2)))
  }

  # crude starting values
  N1 <- sum(pop1); N2 <- sum(pop2)
  t1pos <- (pop1[2,1]+pop1[2,2] + pop2[2,1]+pop2[2,2])/(N1+N2)
  t2pos <- (pop1[1,2]+pop1[2,2] + pop2[1,2]+pop2[2,2])/(N1+N2)
  pclip <- function(p) pmax(0.05, pmin(0.95, p))
  par0 <- c(logit(pclip(t1pos+0.2)), logit(pclip(1-t1pos+0.2)),
            logit(pclip(t2pos+0.2)), logit(pclip(1-t2pos+0.2)),
            logit(0.3), logit(0.7))

  fit <- optim(par0, nll, method = "BFGS", hessian = TRUE, control = list(maxit = 1e4))
  if (fit$convergence != 0) warning("optim did not fully converge (code = ", fit$convergence, ").")

  # transform back
  est <- invlogit(fit$par)
  names(est) <- c("Se1","Sp1","Se2","Sp2","Prev_pop1","Prev_pop2")

  # variance-covariance for logits -> delta to probability scale
  # J = diag( p*(1-p) ) for invlogit componentwise
  gprime <- function(eta) { p <- invlogit(eta); p*(1-p) }
  J <- diag(gprime(fit$par), 6, 6)

  cov_eta <- try(solve(fit$hessian), silent = TRUE)
  if (inherits(cov_eta, "try-error")) {
    # fallback: numerical Hessian if needed
    if (!requireNamespace("numDeriv", quietly = TRUE))
      stop("numDeriv needed for numerical Hessian fallback. Install it or re-run.")
    cov_eta <- try(solve(numDeriv::hessian(nll, fit$par)), silent = TRUE)
  }

  if (inherits(cov_eta, "try-error")) {
    se <- rep(NA_real_, 6)
    cov_p <- matrix(NA_real_, 6, 6)
  } else {
    cov_p <- J %*% cov_eta %*% J
    se <- sqrt(pmax(diag(cov_p), 0))
  }
  names(se) <- names(est)

  # Wald CIs
  z <- qnorm(0.5 + conf_level/2)
  ci <- t(mapply(function(p, s) c(lower = max(0, p - z*s), upper = min(1, p + z*s)),
                 est, se))
  rownames(ci) <- names(est)

  # expected counts at MLE (useful diagnostics)
  ecounts <- function(pi) {
    p <- Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], pi)
    list(p = p,
         pop1 = round(p * N1, 2),
         pop2 = round(p * N2, 2))
  }

  list(
    estimates = est,
    se = se,
    ci = ci,
    cov_logit = cov_eta,
    cov_prob  = cov_p,
    logLik = -fit$value,
    convergence = fit$convergence,
    expected = list(
      pop1 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop1"]) * N1, 2),
      pop2 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop2"]) * N2, 2)
    )
  )
}

# --- Example ---
# pop1 <- matrix(c(180, 20,
#                   25, 75), nrow = 2, byrow = TRUE)
# pop2 <- matrix(c(140, 10,
#                   35, 115), nrow = 2, byrow = TRUE)

#res <- hui_walter_mle_se(pop1, pop2)

```


run the hui and walter

```{r}

res <- hui_walter_mle_se(table(df$test1,test2), table(synthetic_df$test1,synthetic_df$test2))

#se and sp estimates
#res$estimates   # MLEs on probability scale

#Check
list(res$estimates, calc_se_sp(df),calc_se_sp(synthetic_df))

res$se          # standard errors (Wald)
res$ci          # Wald 95% CIs
res$logLik
```

Excellent question â€” this is a classic diagnostic test evaluation problem, especially when both tests are measured on the same subjects.
Youâ€™re asking how to check whether the two tests are conditionally independent given the true disease status (or another conditioning variable).

Letâ€™s unpack it clearly, both conceptually and practically (with R examples).

â¸»

ðŸ§  1. The idea

Two diagnostic tests, say T_1 and T_2, are said to be conditionally independent given disease status D if:

P(T_1, T_2 \mid D) = P(T_1 \mid D) \, P(T_2 \mid D)

This means that once you know whether the subject truly has the disease, the results of the two tests are unrelated.

In words:

Any residual correlation between the tests disappears once you control for the disease.

â¸»

âš™ï¸ 2. What data you need

For each subject:
	â€¢	True disease status: D (0 = no disease, 1 = disease)
	â€¢	Two test results: T1, T2 (binary or continuous)

Letâ€™s handle each case separately.

â¸»

ðŸ§© 3. If both tests are binary

You can test conditional independence within each stratum of the disease status.

ðŸ”¹ Step 1 â€” Create 2Ã—2 tables stratified by disease

library(dplyr)

# example data
set.seed(1)
df <- data.frame(
  D  = rbinom(100, 1, 0.5),
  T1 = rbinom(100, 1, 0.7),
  T2 = rbinom(100, 1, 0.7)
)

# Stratified 2x2 tables
table(df$T1, df$T2, df$D)

ðŸ”¹ Step 2 â€” Test independence within each disease stratum

by(df, df$D, function(sub)
  chisq.test(table(sub$T1, sub$T2), correct = FALSE)
)

If both p-values are not significant, you have no evidence against conditional independence.

Alternatively, you can test the association between T1 and T2 using a Mantelâ€“Haenszel test that controls for D:

mantelhaen.test(table(df$T1, df$T2, df$D))

If the Mantelâ€“Haenszel common odds ratio â‰ˆ 1 (and p > 0.05), that supports conditional independence.

â¸»

ðŸ§® 4. If the tests are continuous

Then you check whether theyâ€™re correlated after conditioning on D â€” essentially, the partial correlation between T_1 and T_2 controlling for D.

library(ppcor)
pcor.test(df$T1, df$T2, df$D, method = "spearman")

If the partial correlation â‰ˆ 0 and not significant, the tests are conditionally independent given disease status.

â¸»

ðŸ“Š 5. If true disease status is unknown (latent class model)

Then you canâ€™t stratify by D, so you need to use a latent class model (LCM) to test conditional independence.

For example, using the poLCA or BayesLCA packages in R:

library(poLCA)

# suppose both T1 and T2 are binary
f <- cbind(T1, T2) ~ 1
fit1 <- poLCA(f, df, nclass = 2, graphs = FALSE, maxiter = 1000)

# Compare with model that allows conditional dependence via direct effect
# (if available, or use log-linear model comparison)

The standard LCM assumption is conditional independence given latent class (disease).
You can test this by comparing a model that includes a local dependence parameter (e.g., log-linear term for T_1 \times T_2) versus one that omits it.

â¸»

âœ… Summary

Case	How to Check Conditional Independence
Binary tests, known disease	Chi-square or Mantelâ€“Haenszel test within each disease group
Continuous tests, known disease	Partial correlation (e.g., Spearman) controlling for disease
Disease status unknown	Latent class model with/without dependence parameter


â¸»

Would you like me to show an R example where we simulate two binary diagnostic tests with dependence and formally test for conditional independence given disease? Thatâ€™s often the best way to see it in practice.


-----------------------

Excellent question â€” this goes right to the heart of latent class modeling (LCM) and why the Huiâ€“Walter model was such an important milestone.
Letâ€™s go step by step through what identifiability means and what conditions are required.

â¸»

ðŸ§  1. What â€œidentifiableâ€ means

A latent class model is identifiable if the modelâ€™s parameters (e.g., prevalences, sensitivities, specificities, or class-conditional probabilities) can be uniquely determined from the observed data distribution.

Formally: different parameter values must not yield the same likelihood for the observed data.

If two or more parameter sets produce identical probabilities for all observable combinations of test results, the model is not identifiable â€” i.e., the data canâ€™t tell them apart.

â¸»

ðŸ§© 2. The general structure of an LCM

For K observed tests and C latent classes:

P(\mathbf{X} = \mathbf{x}) = \sum_{c=1}^C \pi_c \prod_{k=1}^K P(X_k = x_k \mid Z=c)

where:
	â€¢	\pi_c: prevalence (prior probability) of latent class c
	â€¢	P(X_k = x_k \mid Z=c): conditional probability of each test result given class c

â¸»

âš™ï¸ 3. Counting parameters vs. data degrees of freedom

A practical rule of thumb for identifiability is:

The number of free parameters must not exceed the number of independent probabilities in the observed data.

For binary tests and C=2 classes:
	â€¢	Data: K binary tests â†’ 2^K possible response patterns
â‡’ 2^K - 1 independent probabilities (they sum to 1).
	â€¢	Parameters:
	â€¢	1 prevalence (Ï€)
	â€¢	2 Ã— K conditional probabilities (sensitivity & 1-specificity per test)
â‡’ Total = 2K + 1 parameters.

For the model to be identifiable (in principle):
2K + 1 \le 2^K - 1
âœ… For K = 3: 7 \le 7 â†’ just identifiable
âŒ For K = 2: 5 \le 3 â†’ not identifiable

Thatâ€™s why:
	â€¢	2 tests in 1 population â†’ underidentified
	â€¢	3 tests in 1 population â†’ minimally identified
	â€¢	2 tests in 2 populations â†’ 6 data degrees of freedom vs. 6 parameters â†’ just identified (the Huiâ€“Walter case)

â¸»

ðŸŒ 4. Strategies to achieve identifiability

Strategy	How it helps	Example
Add more tests	Increases the number of observable response patterns	â‰¥3 binary tests in one population
Add more populations	Each adds new prevalence but shares test properties	2 tests Ã— 2 populations (Huiâ€“Walter)
Impose equality constraints	Reduces free parameters (e.g., same Se/Sp across groups)	Huiâ€“Walter assumes equal Se/Sp in both populations
Add strong priors (Bayesian)	Adds external information to stabilize estimation	Bayesian LCMs with informative Beta priors
Assume conditional independence	Simplifies joint distribution	Huiâ€“Walter, basic LCMs
Fix or anchor some parameters	Known test performance or perfect test	One test assumed â€œclass standardâ€


â¸»

ðŸ§® 5. Examples

Configuration	Populations	Tests	Parameters	Observable Probabilities	Identifiable?
1 pop, 2 tests	1	2	5	3	âŒ No
2 pops, 2 tests	2	2	6	6	âœ… Yes (Huiâ€“Walter)
1 pop, 3 tests	1	3	7	7	âœ… Yes
2 pops, 3 tests	2	3	8	14	âœ… Over-identified (testable model)


â¸»

ðŸ§­ 6. Conditional dependence caveat

If tests are not conditionally independent, you must add correlation terms (e.g., within-class covariance), which increase parameters.
That typically breaks identifiability unless:
	â€¢	You add more tests or populations, or
	â€¢	You constrain the dependence structure (e.g., shared covariance, equal across classes).

â¸»

âœ… In short:

A latent class model is identifiable when the number of observable independent probabilities â‰¥ number of model parameters,
and the modelâ€™s parameters enter the likelihood in a way that uniquely determines those probabilities.

The minimal identifiable models are:
	â€¢	3 binary tests in one population, or
	â€¢	2 binary tests in 2 populations (Huiâ€“Walter).

â¸»



Notes
	â€¢	SEs are Wald (delta method). For parameters near 0 or 1, consider profile likelihood CIs (I can add a profiler if you want).
	â€¢	Assumptions: 2 pops, 2 binary tests, equal Se/Sp across pops, conditional independence, and Ï€1 â‰  Ï€2 for identifiability.
	â€¢	expected gives expected counts in order (00, 01, 10, 11) for each population under the fitted modelâ€”handy for quick fit checks.






**End of Document**
