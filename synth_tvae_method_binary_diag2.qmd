---
title: "Analysis"
date: "`r Sys.Date()`"
author: "Rich Evans, PhD, PSTAT"
format: pdf
number-sections: true
execute: 
  echo: true
output-file: "analysis_results"
reticulate:
  python: "/Users/richardevans/.virtualenvs/r-reticulate/bin/python"
---

Got it! Here’s a compact, ready-to-run R script that:
	1.	Generates a practice dataset: first column is a binary class (0/1), second and third are continuous diagnostic tests, each correlated with the class (and moderately with each other).
	2.	Uses reticulate to call Python’s SDV TVAE and create a synthetic dataset with similar structure.

Paste this whole script into R. It will install missing Python deps if needed, fit TVAE, and return a synthetic data.frame.



```{r}
# Restart R first!
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")

library(reticulate)
py_config()  # should show ~/.virtualenvs/r-reticulate/bin/python


py_run_string("
import sys, subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])
")


py_install('torch', pip = TRUE)

py_install(c('sdv==1.12.1', 'rdt>=1.10.0', 'copulas>=0.11.0'), pip = TRUE)

pd   <- import("pandas")
sdvT <- import("sdv.tabular")   # <- this is where it previously failed
TVAE <- sdvT$TVAE
pd$`__version__`

```


```{r}
#| label: setup
#| echo: false
#| include: false

# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}

# Set global chunk options
knitr::opts_chunk$set(
  echo = FALSE,         # Show code by default
  warning = FALSE,     # Hide warnings in the output
  message = FALSE,     # Hide messages in the output
  fig.width = 4,       # Default figure width
  fig.height = 2.666,      # Default figure height
  fig.align = "center" # Center align figures
)

# Set seed for reproducibility
#set.seed(1234)

```


Great—here’s a drop-in rewrite where both diagnostic tests are binary (0/1) and the synthetic data are also binary.
Key tweaks vs. your previous script:
	•	The practice data now simulate test1/test2 as Bernoulli variables with class-specific rates.
	•	We explicitly tell SDV (via metadata) that class, test1, and test2 are boolean so it learns/generates 0/1.
	•	Conditional sampling controls the synthetic class proportion (not fixed to the original).



This is the practice "real" data. It is three binary columns, and the first is the class label. 
```{r}

# --- 0) Setup (restart R first if you switched Python) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")


# --- 1) Practice binary data (0/1) ---
#set.seed(123)

generate_real_data <- function(class0 = 250 #count in class =0
                          ,class1= 750 #count in class =1
                          ,p1_0 = 0.15 # 1 - sp test 1
                          ,p1_1 = 0.75 #se test 1
                          ,p2_0 = 0.10 # 1 -sp test 2
                          ,p2_1 = 0.65){ # se test 2
n <- class0+class1
class <- c(rep(0,class0),rep(1,class1)) # prevalence 75%



test1 <- ifelse(class==1, rbinom(n,1,p1_1), rbinom(n,1,p1_0))


test2 <- ifelse(class==1, rbinom(n,1,p2_1), rbinom(n,1,p2_0))

df <- data.frame(
  class = as.integer(class),
  test1 = as.integer(test1),
  test2 = as.integer(test2)
)

return(df)}



if(FALSE){
  
 df <- generate_real_data()
}
```



verify se and sp in df. Note that above the input is 1-sp

```{r}
# SE and SP calculator

# df has columns: class, test1, test2

calc_se_sp <- function(df) {
  gold <- df$class
  
  se_sp <- function(test) {
    TP <- sum(test == 1 & gold == 1)
    FN <- sum(test == 0 & gold == 1)
    TN <- sum(test == 0 & gold == 0)
    FP <- sum(test == 1 & gold == 0)
    c(Sensitivity = TP / (TP + FN),
      Specificity = TN / (TN + FP))
  }
  
  results <- sapply(df[, c("test1", "test2")], se_sp)
  round(results, 3)
}

# Example:

if(FALSE){
 calc_se_sp(df)
}

```


```{r}
calc_se_sp_ci <- function(df, conf.level = 0.95) {
  if (!requireNamespace("binom", quietly = TRUE)) {
    stop("Package 'binom' is required. Please install it using install.packages('binom').")
  }
  
  gold <- df$class
  
  se_sp <- function(test) {
    TP <- sum(test == 1 & gold == 1)
    FN <- sum(test == 0 & gold == 1)
    TN <- sum(test == 0 & gold == 0)
    FP <- sum(test == 1 & gold == 0)
    
    # Sensitivity
    sens <- binom::binom.confint(TP, TP + FN, conf.level = conf.level, methods = "wilson")
    # Specificity
    spec <- binom::binom.confint(TN, TN + FP, conf.level = conf.level, methods = "wilson")
    
    c(
      Sensitivity = sens$mean,
      Sens_Lower = sens$lower,
      Sens_Upper = sens$upper,
      Specificity = spec$mean,
      Spec_Lower = spec$lower,
      Spec_Upper = spec$upper
    )
  }
  
  results <- sapply(df[, c("test1", "test2")], se_sp)
  round(results, 3)
}


if(FALSE){
 calc_se_sp_ci(df)
}
```


This takes the practice "real" data and changes it into another similar dataset
code does generate a synthetic dataset conditional on class=0 and class=1 in the requested proportions.

	2.	For each condition, the sampler:
	•	Fixes class to the requested value (0 or 1).
	•	Builds a conditioning vector (one-hot for the categorical value).
	•	Runs the decoder to generate the other columns conditional on that fixed class.
	•	Repeats until it has exactly num_rows rows for that condition.
	3.	It then concatenates the two blocks (n0 of class 0 and n1 of class 1). Your optional shuffle only randomizes order.


```{r}
tvae_synthesize_binary <- function(
  df,
  n_synth   = 1000,
  p_class1  = 0.30,
  epochs    = 250L,
  validate  = TRUE,
  shuffle   = TRUE,
  return_metrics = TRUE
) {
  stopifnot(all(c("class","test1","test2") %in% names(df)))

  # --- Python imports via reticulate ---
  if (!"reticulate" %in% .packages()) library(reticulate)
  pd          <- import("pandas")
  sdvS        <- import("sdv.single_table")
  sdvM        <- import("sdv.metadata")
  sdvSampling <- import("sdv.sampling")

  # --- SDV metadata: mark binaries as categorical ---
  SingleTableMetadata <- sdvM$SingleTableMetadata
  metadata <- SingleTableMetadata()
  metadata$add_column(column_name = "class", sdtype = "categorical")
  metadata$add_column(column_name = "test1", sdtype = "categorical")
  metadata$add_column(column_name = "test2", sdtype = "categorical")

  # Ensure the input columns are 0/1 integers
  df <- within(df[, c("class","test1","test2")], {
    class <- as.integer(class); test1 <- as.integer(test1); test2 <- as.integer(test2)
  })

  # --- Train TVAE ---
  py_df <- r_to_py(df)
  TVAESynthesizer <- sdvS$TVAESynthesizer
  tvae <- TVAESynthesizer(metadata = metadata
                          , epochs = as.integer(epochs)
                          , batch_size = as.integer(100) #divisable by 10
)
  tvae$fit(py_df)


 training_loss <- tryCatch({
  loss_df <- reticulate::py_to_r(tvae$get_loss_values())
  if (is.data.frame(loss_df) && all(c("epoch","loss") %in% names(loss_df))) loss_df else NULL
}, error = function(e) NULL)
  
#print(reticulate::py_has_attr(tvae, "get_loss_values"))
  
loss_values <- tryCatch({
   py_to_r(tvae$get_loss_values())
 }, error = function(e) NULL)


  
  # if (isTRUE(validate)) {
  #   metadata$validate_data(py_df)
  #   invisible(tvae$sample(as.integer(5L)))  # tiny smoke sample
  # }

  # --- Target size & class mix (with safeguards) ---
  p_class1 <- max(0, min(1, p_class1))         # clamp to [0,1]
  n1 <- round(n_synth * p_class1)
  n0 <- n_synth - n1
  if (p_class1 > 0 && n1 == 0) n1 <- 1         # ensure class 1 present if desired
  if (p_class1 < 1 && n0 == 0) n0 <- 1         # ensure class 0 present if desired
  if (n0 + n1 != n_synth) {                    # fix rounding overflow
    if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
  }

  # --- Conditional sampling ---
  Condition <- sdvSampling$Condition
  conds <- list(
    Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
    Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
  )
  py_syn <- tvae$sample_from_conditions(conds)
  synthetic_df <- py_to_r(py_syn)

  # Optional shuffle to avoid class blocks
  if (isTRUE(shuffle) && nrow(synthetic_df) > 1) {
    synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), , drop = FALSE]
    row.names(synthetic_df) <- NULL
  }

  # Ensure order and integer 0/1 outputs
  synthetic_df <- within(synthetic_df[, c("class","test1","test2")], {
    class <- as.integer(class); test1 <- as.integer(test1); test2 <- as.integer(test2)
  })

  # --- Quick diagnostics ---
  actual_prop <- as.numeric(prop.table(table(synthetic_df$class)))
  names(actual_prop) <- paste0("class_", names(table(synthetic_df$class)))

  diag_list <- list(
    target_class1_prop = p_class1,
    actual_class_props = actual_prop
  )

  # Optionally compute SE/SP if a helper exists in the environment
 # Optionally compute SE/SP if a helper exists in the environment
# Optionally compute SE/SP if a helper exists in the environment
# Optionally compute SE/SP and summarize proportions

  
  diag_list$metrics <- list(
    target_class1_prop = p_class1,
    actual_class_props = prop.table(table(synthetic_df$class)),
    original  = calc_se_sp(df),
    synthetic = calc_se_sp(synthetic_df)
  )


  # Return everything useful
 # then include in the return list
list(
  synthetic_df = synthetic_df,
  diagnostics  = diag_list,
  model        = tvae,
  metadata     = metadata,
  training_loss = loss_values
)
  
}

```

This runs everything


The upshot is that big training sets (n=10,000) work even when there are different class proportion (but not extreme) in the training set and the synthetic set.

Also, a large training set means that fewer epochs are required.


what is interesting is that for small, n=1000 training sets, specificity always seems to fail first, regardless of large (p=0.75) or small (p=0.25). 

```{r}

df <- generate_real_data(class0 = 5000, class1 = 5000)

res <- tvae_synthesize_binary(df, n_synth = 500, p_class1 = 0.5, epochs = 100)
#head(res$synthetic_df)
res$diagnostics

print(list(calc_se_sp_ci(df),calc_se_sp_ci(res$synthetic_df)))

plot(res$training_loss$Epoch, res$training_loss$Loss, type="l", lwd=2)
#res$training_loss$loss
```


GaussianCopulaSynthesizer SEEMS TO DO WORSE THAN TVAE:

do not use

The GaussianCopulaSynthesizer is one of the simpler and most statistically interpretable models in the SDV (Synthetic Data Vault)￼ library.
It generates synthetic tabular data by modeling correlations among columns using a Gaussian copula — a multivariate normal–based dependency structure — while respecting the marginal distributions of each column.

```{r}
library(reticulate)
sdvS <- import("sdv.single_table")
sdvM <- import("sdv.metadata")

SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
metadata$add_column(column_name = "class", sdtype = "categorical")
metadata$add_column(column_name = "test1", sdtype = "categorical")
metadata$add_column(column_name = "test2", sdtype = "categorical")

py_df <- r_to_py(df)

GaussianCopulaSynthesizer <- sdvS$GaussianCopulaSynthesizer
gc <- GaussianCopulaSynthesizer(metadata = metadata)
gc$fit(py_df)

synthetic_df <- py_to_r(gc$sample(as.integer(1000)))


list(calc_se_sp(df), calc_se_sp(synthetic_df))

```

**End of Document**
