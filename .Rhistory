}, error = function(e) {
have_sdv <<- FALSE
})
tryCatch({
import("sdv")
}, error = function(e) {
have_sdv <<- FALSE
})
if (!have_sdv) {
message("Installing Python packages: sdv, pandas...")
reticulate::py_install(c("sdv==1.12.1", "pandas>=1.5"), pip = TRUE)
}
use_virtualenv("~/.virtualenvs/r-reticulate", required = TRUE)
py_config()
pd   <- import("pandas")
sdvT <- import("sdv.tabular")
#| label: setup
#| echo: false
#| include: false
# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}
# Set global chunk options
knitr::opts_chunk$set(
echo = FALSE,         # Show code by default
warning = FALSE,     # Hide warnings in the output
message = FALSE,     # Hide messages in the output
fig.width = 4,       # Default figure width
fig.height = 2.666,      # Default figure height
fig.align = "center" # Center align figures
)
# Set seed for reproducibility
set.seed(1234)
py_config()              # should show ~/.virtualenvs/r-reticulate
# 1) Point reticulate at your virtualenv (before library(reticulate))
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
# 2) Now load reticulate and verify
library(reticulate)
py_config()              # should show ~/.virtualenvs/r-reticulate
py_install(c("pandas", "numpy", "sdv==1.12.1"), pip = TRUE)
py_install(c(
"sdv==1.12.1",   # SDV meta-package (includes tabular models)
"torch",         # required by TVAE
"torchvision",   # often pulled with torch; harmless if unused
"rdt>=1.10.0",   # data transformers used by SDV
"copulas>=0.11.0"
), pip = TRUE)
pd   <- import("pandas")
sdvT <- import("sdv.tabular")   # <- should work now
#| label: setup
#| echo: false
#| include: false
# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}
# Set global chunk options
knitr::opts_chunk$set(
echo = FALSE,         # Show code by default
warning = FALSE,     # Hide warnings in the output
message = FALSE,     # Hide messages in the output
fig.width = 4,       # Default figure width
fig.height = 2.666,      # Default figure height
fig.align = "center" # Center align figures
)
# Set seed for reproducibility
set.seed(1234)
# Restart R first!
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
py_config()  # should show ~/.virtualenvs/r-reticulate/bin/python
py_run_string("
import sys, subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])
")
py_install('torch', pip = TRUE)
py_install(c('sdv==1.12.1', 'rdt>=1.10.0', 'copulas>=0.11.0'), pip = TRUE)
pd   <- import("pandas")
sdvT <- import("sdv.tabular")   # <- this is where it previously failed
reticulate::py_last_error()
reticulate::py_last_error()
# --- 0) Point reticulate at your venv (restart R first) ---
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
# Helpful: see which Python is active
py_config()
# --- 1) Install Python deps if missing ---
py_run_string("
import sys, subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])
")
# Try importing; install if needed
ok <- TRUE
tryCatch(import("sdv"), error = function(e) ok <<- FALSE)
if (!ok) {
# Torch (CPU wheel is safest on macOS)
py_install('torch', pip = TRUE,
pip_options = '--index-url https://download.pytorch.org/whl/cpu')
# SDV core + transformers it uses
py_install(c('sdv', 'rdt', 'copulas', 'pandas', 'numpy'), pip = TRUE)
}
pd   <- import("pandas")
np   <- import("numpy")
sdvS <- import("sdv.single_table")
sdvM <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")
# --- 2) Build a practice dataset in R ---
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
mu0 <- c(0, 0)
mu1 <- c(1.25, 0.9)
Sigma <- matrix(c(1.0, 0.35,
0.35, 1.0), 2, 2)
rmvnorm2 <- function(n, mean, Sigma) {
L <- chol(Sigma)
Z <- matrix(rnorm(2*n), ncol = 2)
sweep(Z %*% L, 2, mean, "+")
}
x <- matrix(NA_real_, nrow = n, ncol = 2)
x[class == 0, ] <- rmvnorm2(sum(class == 0), mu0, Sigma)
x[class == 1, ] <- rmvnorm2(sum(class == 1), mu1, Sigma)
df <- data.frame(
class = as.integer(class),   # fixed label
test1 = x[,1],
test2 = x[,2]
)
# --- 3) Fit TVAE (new SDV API with Metadata) ---
py_df <- r_to_py(df)
Metadata <- sdvM$Metadata
# ---- practice data ----
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
mu0 <- c(0, 0); mu1 <- c(1.25, 0.9)
Sigma <- matrix(c(1, 0.35, 0.35, 1), 2, 2)
rmvnorm2 <- function(n, mean, Sigma){ L <- chol(Sigma); Z <- matrix(rnorm(2*n), ncol=2); sweep(Z %*% L, 2, mean, "+") }
x <- matrix(NA_real_, n, 2)
x[class==0,] <- rmvnorm2(sum(class==0), mu0, Sigma)
x[class==1,] <- rmvnorm2(sum(class==1), mu1, Sigma)
df <- data.frame(class=as.integer(class), test1=x[,1], test2=x[,2])
# ---- Python imports (install if needed) ----
try(import("sdv"), silent=TRUE);  # ensures sdv is there; install via py_install if not
pd  <- import("pandas")
sdvS <- import("sdv.single_table")
sdvM <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")
# ---- metadata via SingleTableMetadata (works across SDV versions) ----
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
py_df <- r_to_py(df)
metadata$detect_from_dataframe(data = py_df)  # single-table autodetect
# ---- TVAE fit ----
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)
tvae$fit(py_df)
# ---- conditional sampling so 'class' is fixed ----
# Try DataFrameCondition (keeps exact sequence). If not present, fall back to Condition blocks.
synthetic_df <- NULL
have_dfcond <- TRUE
DataFrameCondition <- NULL
tryCatch({ DataFrameCondition <<- sdvSampling$DataFrameCondition },
error = function(e) have_dfcond <<- FALSE)
if (have_dfcond) {
# Exact row-by-row fixed class via DataFrameCondition
cond_df <- pd$DataFrame(dict(class = r_to_py(df$class)))
cond <- DataFrameCondition(dataframe = cond_df)
py_syn <- tvae$sample_from_conditions(list(cond))
synthetic_df <- py_to_r(py_syn)
} else {
# Fallback: two bulk conditions, then restore the original class order
Condition <- sdvSampling$Condition
n0 <- sum(df$class == 0L); n1 <- sum(df$class == 1L)
c0 <- Condition(num_rows = as.integer(n0), column_values = dict(class = 0L))
c1 <- Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
py_syn <- tvae$sample_from_conditions(list(c0, c1))
synthetic_df <- py_to_r(py_syn)
# ensure columns and restore exact original class vector (sequence unchanged)
synthetic_df <- synthetic_df[, c("class","test1","test2")]
synthetic_df$class <- df$class
}
# ---- sanity checks (optional) ----
cat("\nhead(original):\n"); print(head(df))
cat("\nhead(synthetic):\n"); print(head(synthetic_df))
cat("\ncor(original):\n"); print(cor(df[c("test1","test2")]))
cat("\ncor(synthetic):\n"); print(cor(synthetic_df[c("test1","test2")]))
cat("\nclass balance (orig vs synth):\n")
print(prop.table(table(df$class))); print(prop.table(table(synthetic_df$class)))
head(df)
head(df_syth)
head(synthetic_df)
# --- 0) Environment (set once per session) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
# Imports (assumes sdv/pandas/numpy/torch are installed in the chosen Python)
pd            <- import("pandas")
sdvS          <- import("sdv.single_table")
sdvM          <- import("sdv.metadata")
sdvSampling   <- import("sdv.sampling")
# --- 1) Practice dataset (same as before) ---
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
mu0 <- c(0, 0); mu1 <- c(1.25, 0.9)
Sigma <- matrix(c(1, 0.35, 0.35, 1), 2, 2)
rmvnorm2 <- function(n, mean, Sigma){
L <- chol(Sigma); Z <- matrix(rnorm(2*n), ncol=2)
sweep(Z %*% L, 2, mean, "+")
}
x <- matrix(NA_real_, nrow=n, ncol=2)
x[class==0,] <- rmvnorm2(sum(class==0), mu0, Sigma)
x[class==1,] <- rmvnorm2(sum(class==1), mu1, Sigma)
df <- data.frame(
class = as.integer(class),
test1 = x[,1],
test2 = x[,2]
)
# --- 2) Fit TVAE (SingleTableMetadata for broad SDV compatibility) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
py_df <- r_to_py(df)
metadata$detect_from_dataframe(data = py_df)
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)
tvae$fit(py_df)
# --- 3) Choose synthetic size and class proportion (edit these) ---
n_synth   <- 1000        # how many synthetic rows you want
p_class1  <- 0.70        # desired proportion for class==1 (e.g., 70%)
# safety: keep within [0,1]
p_class1  <- max(0, min(1, p_class1))
# counts (ensure at least 1 row per class if both are nonzero proportions)
n1 <- round(n_synth * p_class1)
n0 <- n_synth - n1
if (p_class1 > 0 && n1 == 0) n1 <- 1
if (p_class1 < 1 && n0 == 0) n0 <- 1
# adjust back to exact n_synth if we changed something
if (n0 + n1 != n_synth) {
# nudge the larger one to fix total
if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
}
# --- 4) Conditional sampling for the requested mix (do NOT overwrite class) ---
Condition <- sdvSampling$Condition
conds <- list(
Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
)
py_syn <- tvae$sample_from_conditions(conds)
synthetic_df <- py_to_r(py_syn)
# Optional: shuffle rows so classes aren’t in two big blocks
set.seed(42)
synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), ]
row.names(synthetic_df) <- NULL
# Ensure column order
synthetic_df <- synthetic_df[, c("class","test1","test2")]
# --- 5) Quick checks (optional) ---
cat("\nTarget class1 proportion:", p_class1, "\n")
cat("Actual synthetic proportions:\n")
print(prop.table(table(synthetic_df$class)))
cat("\nCor(original test1,test2):\n"); print(cor(df[c("test1","test2")]))
# --- 0) Environment (once per session) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
# Imports (assumes sdv/pandas/numpy/torch installed in chosen Python)
pd           <- import("pandas")
sdvS         <- import("sdv.single_table")
sdvM         <- import("sdv.metadata")
sdvSampling  <- import("sdv.sampling")
# --- 1) Practice dataset: binary tests conditioned on class ----
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)  # disease/condition label
# Class-specific Bernoulli rates (edit as you like)
p1_0 <- 0.15  # P(test1=1 | class=0)
p2_0 <- 0.10  # P(test2=1 | class=0)
p1_1 <- 0.75  # P(test1=1 | class=1)
p2_1 <- 0.65  # P(test2=1 | class=1)
# Generate binary tests (conditionally independent given class by default)
test1 <- ifelse(class == 1, rbinom(n, 1, p1_1), rbinom(n, 1, p1_0))
test2 <- ifelse(class == 1, rbinom(n, 1, p2_1), rbinom(n, 1, p2_0))
df <- data.frame(
class = as.integer(class),
test1 = as.integer(test1),
test2 = as.integer(test2)
)
# --- 2) Fit TVAE with explicit boolean metadata (ensures binary output) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
# Define columns and sdtypes explicitly as boolean
metadata$add_column(name = "class", sdtype = "boolean")
metadata$add_column(name = "test1", sdtype = "boolean")
# --- 2) Fit TVAE with explicit boolean metadata (ensures binary output) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
# Define columns and sdtypes explicitly as boolean
metadata$add_column(name = "class", sdtype = "boolean")
# Define columns and sdtypes explicitly as boolean
metadata$add_column(column_name = "class", sdtype = "boolean")
metadata$add_column(column_name = "test1", sdtype = "boolean")
metadata$add_column(column_name = "test2", sdtype = "boolean")
py_df <- r_to_py(df)
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)
# Make the R columns logical (TRUE/FALSE) instead of 0/1
df$class <- as.logical(df$class)
df$test1 <- as.logical(df$test1)
df$test2 <- as.logical(df$test2)
py_df <- r_to_py(df)  # now matches boolean metadata
# ... after fitting, when sampling with a target mix:
Condition <- sdvSampling$Condition
conds <- list(
Condition(num_rows = as.integer(n0), column_values = dict(class = FALSE)),
Condition(num_rows = as.integer(n1), column_values = dict(class = TRUE))
)
py_syn <- tvae$sample_from_conditions(conds)
reticulate::py_last_error()
# ---- 0) Setup ----
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
pd           <- import("pandas")
sdvS         <- import("sdv.single_table")
sdvM         <- import("sdv.metadata")
sdvSampling  <- import("sdv.sampling")
# ---- 1) Practice binary data (0/1 integers) ----
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
p1_0 <- 0.15; p2_0 <- 0.10; p1_1 <- 0.75; p2_1 <- 0.65
test1 <- ifelse(class==1, rbinom(n,1,p1_1), rbinom(n,1,p1_0))
test2 <- ifelse(class==1, rbinom(n,1,p2_1), rbinom(n,1,p2_0))
df <- data.frame(class=as.integer(class), test1=as.integer(test1), test2=as.integer(test2))
# ---- 2) Metadata: declare all three as categorical (0/1) ----
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
metadata$add_column(column_name="class", sdtype="categorical")
metadata$add_column(column_name="test1", sdtype="categorical")
metadata$add_column(column_name="test2", sdtype="categorical")
py_df <- r_to_py(df)
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata=metadata, epochs=as.integer(250L))
tvae$fit(py_df)  # must complete successfully
tvae$fit(py_df)  # must complete successfully
# ---- 3) Choose synthetic size and class mix ----
n_synth  <- 1000
p_class1 <- 0.70
p_class1 <- max(0, min(1, p_class1))
n1 <- round(n_synth * p_class1); n0 <- n_synth - n1
if (p_class1>0 && n1==0) n1 <- 1
if (p_class1<1 && n0==0) n0 <- 1
if (n0+n1 != n_synth) {
if (n1>n0) n1 <- n1 - ((n0+n1)-n_synth) else n0 <- n0 - ((n0+n1)-n_synth)
}
# ---- 4) Use DataFrameCondition (most stable) with an exact class vector ----
DataFrameCondition <- sdvSampling$DataFrameCondition
reticulate::py_last_error()
# --- 0) Setup (restart R first if you switched Python) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)
pd          <- import("pandas")
sdvS        <- import("sdv.single_table")
sdvM        <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")
# --- 1) Practice binary data (0/1) ---
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
p1_0 <- 0.15; p2_0 <- 0.10; p1_1 <- 0.75; p2_1 <- 0.65
test1 <- ifelse(class==1, rbinom(n,1,p1_1), rbinom(n,1,p1_0))
test2 <- ifelse(class==1, rbinom(n,1,p2_1), rbinom(n,1,p2_0))
df <- data.frame(
class = as.integer(class),
test1 = as.integer(test1),
test2 = as.integer(test2)
)
# --- 2) Metadata: declare 0/1 as categorical (robust across versions) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
metadata$add_column(column_name = "class", sdtype = "categorical")
metadata$add_column(column_name = "test1", sdtype = "categorical")
metadata$add_column(column_name = "test2", sdtype = "categorical")
py_df <- r_to_py(df)
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)
# (Optional) Validate training data against metadata & do a tiny smoke sample
metadata$validate_data(py_df)
invisible(tvae$sample(as.integer(5L)))
# --- 3) Target synthetic size & class mix ---
n_synth  <- 1000
p_class1 <- 0.70
p_class1 <- max(0, min(1, p_class1))
n1 <- round(n_synth * p_class1); n0 <- n_synth - n1
if (p_class1 > 0 && n1 == 0) n1 <- 1
if (p_class1 < 1 && n0 == 0) n0 <- 1
if (n0 + n1 != n_synth) {
if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
}
# --- 4) Conditional sampling with Condition blocks (no DataFrameCondition) ---
Condition <- sdvSampling$Condition
conds <- list(
Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
)
py_syn <- tvae$sample_from_conditions(conds)
synthetic_df <- py_to_r(py_syn)
# Shuffle to avoid two big class blocks (optional)
set.seed(42)
synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), ]
row.names(synthetic_df) <- NULL
# Ensure column order and 0/1 integers
synthetic_df <- within(synthetic_df[, c("class","test1","test2")], {
class <- as.integer(class); test1 <- as.integer(test1); test2 <- as.integer(test2)
})
# --- 5) Quick checks ---
cat("\nTarget class1 proportion:", p_class1, "\n")
print(prop.table(table(synthetic_df$class)))
# Hui–Walter MLE (2 tests × 2 populations), with SEs and Wald CIs
# pop1, pop2: 2x2 integer matrices (rows T1=0/1, cols T2=0/1)
hui_walter_mle_se <- function(pop1, pop2, conf_level = 0.95) {
stopifnot(is.matrix(pop1), is.matrix(pop2),
all(dim(pop1) == c(2,2)), all(dim(pop2) == c(2,2)))
invlogit <- function(eta) 1/(1+exp(-eta))
logit    <- function(p) log(p/(1-p))
# Likelihood pieces
Pcells <- function(se1, sp1, se2, sp2, pi) {
c(
p00 = pi*(1-se1)*(1-se2) + (1-pi)*sp1*sp2,
p01 = pi*(1-se1)*se2     + (1-pi)*sp1*(1-sp2),
p10 = pi*se1*(1-se2)     + (1-pi)*(1-sp1)*sp2,
p11 = pi*se1*se2         + (1-pi)*(1-sp1)*(1-sp2)
)
}
nll <- function(par) {
se1 <- invlogit(par[1]); sp1 <- invlogit(par[2])
se2 <- invlogit(par[3]); sp2 <- invlogit(par[4])
pi1 <- invlogit(par[5]); pi2 <- invlogit(par[6])
p1 <- pmax(Pcells(se1, sp1, se2, sp2, pi1), 1e-12)
p2 <- pmax(Pcells(se1, sp1, se2, sp2, pi2), 1e-12)
# counts in (00,01,10,11) order
x1 <- c(pop1[1,1], pop1[1,2], pop1[2,1], pop1[2,2])
x2 <- c(pop2[1,1], pop2[1,2], pop2[2,1], pop2[2,2])
-(sum(x1*log(p1)) + sum(x2*log(p2)))
}
# crude starting values
N1 <- sum(pop1); N2 <- sum(pop2)
t1pos <- (pop1[2,1]+pop1[2,2] + pop2[2,1]+pop2[2,2])/(N1+N2)
t2pos <- (pop1[1,2]+pop1[2,2] + pop2[1,2]+pop2[2,2])/(N1+N2)
pclip <- function(p) pmax(0.05, pmin(0.95, p))
par0 <- c(logit(pclip(t1pos+0.2)), logit(pclip(1-t1pos+0.2)),
logit(pclip(t2pos+0.2)), logit(pclip(1-t2pos+0.2)),
logit(0.3), logit(0.7))
fit <- optim(par0, nll, method = "BFGS", hessian = TRUE, control = list(maxit = 1e4))
if (fit$convergence != 0) warning("optim did not fully converge (code = ", fit$convergence, ").")
# transform back
est <- invlogit(fit$par)
names(est) <- c("Se1","Sp1","Se2","Sp2","Prev_pop1","Prev_pop2")
# variance-covariance for logits -> delta to probability scale
# J = diag( p*(1-p) ) for invlogit componentwise
gprime <- function(eta) { p <- invlogit(eta); p*(1-p) }
J <- diag(gprime(fit$par), 6, 6)
cov_eta <- try(solve(fit$hessian), silent = TRUE)
if (inherits(cov_eta, "try-error")) {
# fallback: numerical Hessian if needed
if (!requireNamespace("numDeriv", quietly = TRUE))
stop("numDeriv needed for numerical Hessian fallback. Install it or re-run.")
cov_eta <- try(solve(numDeriv::hessian(nll, fit$par)), silent = TRUE)
}
if (inherits(cov_eta, "try-error")) {
se <- rep(NA_real_, 6)
cov_p <- matrix(NA_real_, 6, 6)
} else {
cov_p <- J %*% cov_eta %*% J
se <- sqrt(pmax(diag(cov_p), 0))
}
names(se) <- names(est)
# Wald CIs
z <- qnorm(0.5 + conf_level/2)
ci <- t(mapply(function(p, s) c(lower = max(0, p - z*s), upper = min(1, p + z*s)),
est, se))
rownames(ci) <- names(est)
# expected counts at MLE (useful diagnostics)
ecounts <- function(pi) {
p <- Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], pi)
list(p = p,
pop1 = round(p * N1, 2),
pop2 = round(p * N2, 2))
}
list(
estimates = est,
se = se,
ci = ci,
cov_logit = cov_eta,
cov_prob  = cov_p,
logLik = -fit$value,
convergence = fit$convergence,
expected = list(
pop1 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop1"]) * N1, 2),
pop2 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop2"]) * N2, 2)
)
)
}
# --- Example ---
pop1 <- matrix(c(180, 20,
25, 75), nrow = 2, byrow = TRUE)
pop2 <- matrix(c(140, 10,
35, 115), nrow = 2, byrow = TRUE)
res <- hui_walter_mle_se(pop1, pop2)
res$estimates   # MLEs on probability scale
res$se          # standard errors (Wald)
res$ci          # Wald 95% CIs
res$logLik
