---
title: "Analysis"
date: "`r Sys.Date()`"
author: "Rich Evans, PhD, PSTAT"
format: pdf
number-sections: true
execute: 
  echo: true
output-file: "analysis_results"
reticulate:
  python: "/Users/richardevans/.virtualenvs/r-reticulate/bin/python"
---

Got it! Hereâ€™s a compact, ready-to-run R script that:
	1.	Generates a practice dataset: first column is a binary class (0/1), second and third are continuous diagnostic tests, each correlated with the class (and moderately with each other).
	2.	Uses reticulate to call Pythonâ€™s SDV TVAE and create a synthetic dataset with similar structure.

Paste this whole script into R. It will install missing Python deps if needed, fit TVAE, and return a synthetic data.frame.



```{r}
# Restart R first!
Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")

library(reticulate)
py_config()  # should show ~/.virtualenvs/r-reticulate/bin/python


py_run_string("
import sys, subprocess
subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])
")


py_install('torch', pip = TRUE)

py_install(c('sdv==1.12.1', 'rdt>=1.10.0', 'copulas>=0.11.0'), pip = TRUE)

pd   <- import("pandas")
sdvT <- import("sdv.tabular")   # <- this is where it previously failed
TVAE <- sdvT$TVAE
pd$`__version__`

```


```{r}
#| label: setup
#| echo: false
#| include: false

# a little template for installing. BUT perhaps better to use pacman for many packages
# if (!requireNamespace("bnlearn", quietly = TRUE)) {
#   install.packages("bnlearn", quietly = TRUE)}

# Set global chunk options
knitr::opts_chunk$set(
  echo = FALSE,         # Show code by default
  warning = FALSE,     # Hide warnings in the output
  message = FALSE,     # Hide messages in the output
  fig.width = 4,       # Default figure width
  fig.height = 2.666,      # Default figure height
  fig.align = "center" # Center align figures
)

# Set seed for reproducibility
set.seed(1234)

```



```{r}
# --- point reticulate to your env first (restart R if needed) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")


# ---- practice data ----
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
mu0 <- c(0, 0); mu1 <- c(1.25, 0.9)
Sigma <- matrix(c(1, 0.35, 0.35, 1), 2, 2)
rmvnorm2 <- function(n, mean, Sigma){ L <- chol(Sigma); Z <- matrix(rnorm(2*n), ncol=2); sweep(Z %*% L, 2, mean, "+") }
x <- matrix(NA_real_, n, 2)
x[class==0,] <- rmvnorm2(sum(class==0), mu0, Sigma)
x[class==1,] <- rmvnorm2(sum(class==1), mu1, Sigma)
df <- data.frame(class=as.integer(class), test1=x[,1], test2=x[,2])

# ---- Python imports (install if needed) ----
try(import("sdv"), silent=TRUE);  # ensures sdv is there; install via py_install if not
pd  <- import("pandas")
sdvS <- import("sdv.single_table")
sdvM <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")

# ---- metadata via SingleTableMetadata (works across SDV versions) ----
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
py_df <- r_to_py(df)
metadata$detect_from_dataframe(data = py_df)  # single-table autodetect

# ---- TVAE fit ----
TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)

# ---- conditional sampling so 'class' is fixed ----
# Try DataFrameCondition (keeps exact sequence). If not present, fall back to Condition blocks.
synthetic_df <- NULL
have_dfcond <- TRUE
DataFrameCondition <- NULL
tryCatch({ DataFrameCondition <<- sdvSampling$DataFrameCondition },
         error = function(e) have_dfcond <<- FALSE)

if (have_dfcond) {
  # Exact row-by-row fixed class via DataFrameCondition
  cond_df <- pd$DataFrame(dict(class = r_to_py(df$class)))
  cond <- DataFrameCondition(dataframe = cond_df)
  py_syn <- tvae$sample_from_conditions(list(cond))
  synthetic_df <- py_to_r(py_syn)
} else {
  # Fallback: two bulk conditions, then restore the original class order
  Condition <- sdvSampling$Condition
  n0 <- sum(df$class == 0L); n1 <- sum(df$class == 1L)
  c0 <- Condition(num_rows = as.integer(n0), column_values = dict(class = 0L))
  c1 <- Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
  py_syn <- tvae$sample_from_conditions(list(c0, c1))
  synthetic_df <- py_to_r(py_syn)
  # ensure columns and restore exact original class vector (sequence unchanged)
  synthetic_df <- synthetic_df[, c("class","test1","test2")]
  synthetic_df$class <- df$class
}

# ---- sanity checks (optional) ----
cat("\nhead(original):\n"); print(head(df))
cat("\nhead(synthetic):\n"); print(head(synthetic_df))
cat("\ncor(original):\n"); print(cor(df[c("test1","test2")]))
cat("\ncor(synthetic):\n"); print(cor(synthetic_df[c("test1","test2")]))
cat("\nclass balance (orig vs synth):\n")
print(prop.table(table(df$class))); print(prop.table(table(synthetic_df$class)))

head(df)

head(synthetic_df)
```




How to use
	â€¢	Change n_synth to the number of rows you want.
	â€¢	Set p_class1 to your desired proportion of class==1 (e.g., 0.30, 0.5, 0.85, etc.).
	â€¢	The code does not overwrite the synthetic class column; itâ€™s generated via conditional sampling to match your target mix.

If you later want to condition on multiple variables (e.g., class and sex), create a set of Condition(...) objects for each combination with desired num_rows, and pass them in the list to sample_from_conditions().

```{r}
# --- 0) Environment (set once per session) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)

# Imports (assumes sdv/pandas/numpy/torch are installed in the chosen Python)
pd            <- import("pandas")
sdvS          <- import("sdv.single_table")
sdvM          <- import("sdv.metadata")
sdvSampling   <- import("sdv.sampling")

# --- 1) Practice dataset (same as before) ---
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)

mu0 <- c(0, 0); mu1 <- c(1.25, 0.9)
Sigma <- matrix(c(1, 0.35, 0.35, 1), 2, 2)
rmvnorm2 <- function(n, mean, Sigma){
  L <- chol(Sigma); Z <- matrix(rnorm(2*n), ncol=2)
  sweep(Z %*% L, 2, mean, "+")
}
x <- matrix(NA_real_, nrow=n, ncol=2)
x[class==0,] <- rmvnorm2(sum(class==0), mu0, Sigma)
x[class==1,] <- rmvnorm2(sum(class==1), mu1, Sigma)

df <- data.frame(
  class = as.integer(class),
  test1 = x[,1],
  test2 = x[,2]
)

# --- 2) Fit TVAE (SingleTableMetadata for broad SDV compatibility) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
py_df <- r_to_py(df)
metadata$detect_from_dataframe(data = py_df)

TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)

# --- 3) Choose synthetic size and class proportion (edit these) ---
n_synth   <- 1000        # how many synthetic rows you want
p_class1  <- 0.70        # desired proportion for class==1 (e.g., 70%)
# safety: keep within [0,1]
p_class1  <- max(0, min(1, p_class1))

# counts (ensure at least 1 row per class if both are nonzero proportions)
n1 <- round(n_synth * p_class1)
n0 <- n_synth - n1
if (p_class1 > 0 && n1 == 0) n1 <- 1
if (p_class1 < 1 && n0 == 0) n0 <- 1
# adjust back to exact n_synth if we changed something
if (n0 + n1 != n_synth) {
  # nudge the larger one to fix total
  if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
}

# --- 4) Conditional sampling for the requested mix (do NOT overwrite class) ---
Condition <- sdvSampling$Condition
conds <- list(
  Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
  Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
)

py_syn <- tvae$sample_from_conditions(conds)
synthetic_df <- py_to_r(py_syn)

# Optional: shuffle rows so classes arenâ€™t in two big blocks
set.seed(42)
synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), ]
row.names(synthetic_df) <- NULL

# Ensure column order
synthetic_df <- synthetic_df[, c("class","test1","test2")]

# --- 5) Quick checks (optional) ---
cat("\nTarget class1 proportion:", p_class1, "\n")
cat("Actual synthetic proportions:\n")
print(prop.table(table(synthetic_df$class)))

cat("\nCor(original test1,test2):\n"); print(cor(df[c("test1","test2")]))
cat("\nCor(synthetic test1,test2):\n"); print(cor(synthetic_df[c("test1","test2")]))

# Objects:
# - df            : original practice dataset
# - synthetic_df  : synthetic dataset with your chosen class mix (not fixed to df)
```


Greatâ€”hereâ€™s a drop-in rewrite where both diagnostic tests are binary (0/1) and the synthetic data are also binary.
Key tweaks vs. your previous script:
	â€¢	The practice data now simulate test1/test2 as Bernoulli variables with class-specific rates.
	â€¢	We explicitly tell SDV (via metadata) that class, test1, and test2 are boolean so it learns/generates 0/1.
	â€¢	Conditional sampling controls the synthetic class proportion (not fixed to the original).


```{r}

# --- 0) Setup (restart R first if you switched Python) ---
# Sys.setenv(RETICULATE_PYTHON="~/.virtualenvs/r-reticulate/bin/python")
library(reticulate)

pd          <- import("pandas")
sdvS        <- import("sdv.single_table")
sdvM        <- import("sdv.metadata")
sdvSampling <- import("sdv.sampling")

# --- 1) Practice binary data (0/1) ---
set.seed(123)
n <- 1000
class <- rbinom(n, 1, 0.5)
p1_0 <- 0.15; p2_0 <- 0.10; p1_1 <- 0.75; p2_1 <- 0.65
test1 <- ifelse(class==1, rbinom(n,1,p1_1), rbinom(n,1,p1_0))
test2 <- ifelse(class==1, rbinom(n,1,p2_1), rbinom(n,1,p2_0))

df <- data.frame(
  class = as.integer(class),
  test1 = as.integer(test1),
  test2 = as.integer(test2)
)

# --- 2) Metadata: declare 0/1 as categorical (robust across versions) ---
SingleTableMetadata <- sdvM$SingleTableMetadata
metadata <- SingleTableMetadata()
metadata$add_column(column_name = "class", sdtype = "categorical")
metadata$add_column(column_name = "test1", sdtype = "categorical")
metadata$add_column(column_name = "test2", sdtype = "categorical")

py_df <- r_to_py(df)

TVAESynthesizer <- sdvS$TVAESynthesizer
tvae <- TVAESynthesizer(metadata = metadata, epochs = as.integer(250))
tvae$fit(py_df)

# (Optional) Validate training data against metadata & do a tiny smoke sample
metadata$validate_data(py_df)
invisible(tvae$sample(as.integer(5L)))

# --- 3) Target synthetic size & class mix ---
n_synth  <- 1000
p_class1 <- 0.70
p_class1 <- max(0, min(1, p_class1))
n1 <- round(n_synth * p_class1); n0 <- n_synth - n1
if (p_class1 > 0 && n1 == 0) n1 <- 1
if (p_class1 < 1 && n0 == 0) n0 <- 1
if (n0 + n1 != n_synth) {
  if (n1 > n0) n1 <- n1 - ((n0 + n1) - n_synth) else n0 <- n0 - ((n0 + n1) - n_synth)
}

# --- 4) Conditional sampling with Condition blocks (no DataFrameCondition) ---
Condition <- sdvSampling$Condition
conds <- list(
  Condition(num_rows = as.integer(n0), column_values = dict(class = 0L)),
  Condition(num_rows = as.integer(n1), column_values = dict(class = 1L))
)

py_syn <- tvae$sample_from_conditions(conds)
synthetic_df <- py_to_r(py_syn)

# Shuffle to avoid two big class blocks (optional)
set.seed(42)
synthetic_df <- synthetic_df[sample.int(nrow(synthetic_df)), ]
row.names(synthetic_df) <- NULL

# Ensure column order and 0/1 integers
synthetic_df <- within(synthetic_df[, c("class","test1","test2")], {
  class <- as.integer(class); test1 <- as.integer(test1); test2 <- as.integer(test2)
})

# --- 5) Quick checks ---
cat("\nTarget class1 proportion:", p_class1, "\n")
print(prop.table(table(synthetic_df$class)))



```



Excellent question â€” this is a classic diagnostic test evaluation problem, especially when both tests are measured on the same subjects.
Youâ€™re asking how to check whether the two tests are conditionally independent given the true disease status (or another conditioning variable).

Letâ€™s unpack it clearly, both conceptually and practically (with R examples).

â¸»

ðŸ§  1. The idea

Two diagnostic tests, say T_1 and T_2, are said to be conditionally independent given disease status D if:

P(T_1, T_2 \mid D) = P(T_1 \mid D) \, P(T_2 \mid D)

This means that once you know whether the subject truly has the disease, the results of the two tests are unrelated.

In words:

Any residual correlation between the tests disappears once you control for the disease.

â¸»

âš™ï¸ 2. What data you need

For each subject:
	â€¢	True disease status: D (0 = no disease, 1 = disease)
	â€¢	Two test results: T1, T2 (binary or continuous)

Letâ€™s handle each case separately.

â¸»

ðŸ§© 3. If both tests are binary

You can test conditional independence within each stratum of the disease status.

ðŸ”¹ Step 1 â€” Create 2Ã—2 tables stratified by disease

library(dplyr)

# example data
set.seed(1)
df <- data.frame(
  D  = rbinom(100, 1, 0.5),
  T1 = rbinom(100, 1, 0.7),
  T2 = rbinom(100, 1, 0.7)
)

# Stratified 2x2 tables
table(df$T1, df$T2, df$D)

ðŸ”¹ Step 2 â€” Test independence within each disease stratum

by(df, df$D, function(sub)
  chisq.test(table(sub$T1, sub$T2), correct = FALSE)
)

If both p-values are not significant, you have no evidence against conditional independence.

Alternatively, you can test the association between T1 and T2 using a Mantelâ€“Haenszel test that controls for D:

mantelhaen.test(table(df$T1, df$T2, df$D))

If the Mantelâ€“Haenszel common odds ratio â‰ˆ 1 (and p > 0.05), that supports conditional independence.

â¸»

ðŸ§® 4. If the tests are continuous

Then you check whether theyâ€™re correlated after conditioning on D â€” essentially, the partial correlation between T_1 and T_2 controlling for D.

library(ppcor)
pcor.test(df$T1, df$T2, df$D, method = "spearman")

If the partial correlation â‰ˆ 0 and not significant, the tests are conditionally independent given disease status.

â¸»

ðŸ“Š 5. If true disease status is unknown (latent class model)

Then you canâ€™t stratify by D, so you need to use a latent class model (LCM) to test conditional independence.

For example, using the poLCA or BayesLCA packages in R:

library(poLCA)

# suppose both T1 and T2 are binary
f <- cbind(T1, T2) ~ 1
fit1 <- poLCA(f, df, nclass = 2, graphs = FALSE, maxiter = 1000)

# Compare with model that allows conditional dependence via direct effect
# (if available, or use log-linear model comparison)

The standard LCM assumption is conditional independence given latent class (disease).
You can test this by comparing a model that includes a local dependence parameter (e.g., log-linear term for T_1 \times T_2) versus one that omits it.

â¸»

âœ… Summary

Case	How to Check Conditional Independence
Binary tests, known disease	Chi-square or Mantelâ€“Haenszel test within each disease group
Continuous tests, known disease	Partial correlation (e.g., Spearman) controlling for disease
Disease status unknown	Latent class model with/without dependence parameter


â¸»

Would you like me to show an R example where we simulate two binary diagnostic tests with dependence and formally test for conditional independence given disease? Thatâ€™s often the best way to see it in practice.


-----------------------

Excellent question â€” this goes right to the heart of latent class modeling (LCM) and why the Huiâ€“Walter model was such an important milestone.
Letâ€™s go step by step through what identifiability means and what conditions are required.

â¸»

ðŸ§  1. What â€œidentifiableâ€ means

A latent class model is identifiable if the modelâ€™s parameters (e.g., prevalences, sensitivities, specificities, or class-conditional probabilities) can be uniquely determined from the observed data distribution.

Formally: different parameter values must not yield the same likelihood for the observed data.

If two or more parameter sets produce identical probabilities for all observable combinations of test results, the model is not identifiable â€” i.e., the data canâ€™t tell them apart.

â¸»

ðŸ§© 2. The general structure of an LCM

For K observed tests and C latent classes:

P(\mathbf{X} = \mathbf{x}) = \sum_{c=1}^C \pi_c \prod_{k=1}^K P(X_k = x_k \mid Z=c)

where:
	â€¢	\pi_c: prevalence (prior probability) of latent class c
	â€¢	P(X_k = x_k \mid Z=c): conditional probability of each test result given class c

â¸»

âš™ï¸ 3. Counting parameters vs. data degrees of freedom

A practical rule of thumb for identifiability is:

The number of free parameters must not exceed the number of independent probabilities in the observed data.

For binary tests and C=2 classes:
	â€¢	Data: K binary tests â†’ 2^K possible response patterns
â‡’ 2^K - 1 independent probabilities (they sum to 1).
	â€¢	Parameters:
	â€¢	1 prevalence (Ï€)
	â€¢	2 Ã— K conditional probabilities (sensitivity & 1-specificity per test)
â‡’ Total = 2K + 1 parameters.

For the model to be identifiable (in principle):
2K + 1 \le 2^K - 1
âœ… For K = 3: 7 \le 7 â†’ just identifiable
âŒ For K = 2: 5 \le 3 â†’ not identifiable

Thatâ€™s why:
	â€¢	2 tests in 1 population â†’ underidentified
	â€¢	3 tests in 1 population â†’ minimally identified
	â€¢	2 tests in 2 populations â†’ 6 data degrees of freedom vs. 6 parameters â†’ just identified (the Huiâ€“Walter case)

â¸»

ðŸŒ 4. Strategies to achieve identifiability

Strategy	How it helps	Example
Add more tests	Increases the number of observable response patterns	â‰¥3 binary tests in one population
Add more populations	Each adds new prevalence but shares test properties	2 tests Ã— 2 populations (Huiâ€“Walter)
Impose equality constraints	Reduces free parameters (e.g., same Se/Sp across groups)	Huiâ€“Walter assumes equal Se/Sp in both populations
Add strong priors (Bayesian)	Adds external information to stabilize estimation	Bayesian LCMs with informative Beta priors
Assume conditional independence	Simplifies joint distribution	Huiâ€“Walter, basic LCMs
Fix or anchor some parameters	Known test performance or perfect test	One test assumed â€œgold standardâ€


â¸»

ðŸ§® 5. Examples

Configuration	Populations	Tests	Parameters	Observable Probabilities	Identifiable?
1 pop, 2 tests	1	2	5	3	âŒ No
2 pops, 2 tests	2	2	6	6	âœ… Yes (Huiâ€“Walter)
1 pop, 3 tests	1	3	7	7	âœ… Yes
2 pops, 3 tests	2	3	8	14	âœ… Over-identified (testable model)


â¸»

ðŸ§­ 6. Conditional dependence caveat

If tests are not conditionally independent, you must add correlation terms (e.g., within-class covariance), which increase parameters.
That typically breaks identifiability unless:
	â€¢	You add more tests or populations, or
	â€¢	You constrain the dependence structure (e.g., shared covariance, equal across classes).

â¸»

âœ… In short:

A latent class model is identifiable when the number of observable independent probabilities â‰¥ number of model parameters,
and the modelâ€™s parameters enter the likelihood in a way that uniquely determines those probabilities.

The minimal identifiable models are:
	â€¢	3 binary tests in one population, or
	â€¢	2 binary tests in 2 populations (Huiâ€“Walter).

â¸»



Notes
	â€¢	SEs are Wald (delta method). For parameters near 0 or 1, consider profile likelihood CIs (I can add a profiler if you want).
	â€¢	Assumptions: 2 pops, 2 binary tests, equal Se/Sp across pops, conditional independence, and Ï€1 â‰  Ï€2 for identifiability.
	â€¢	expected gives expected counts in order (00, 01, 10, 11) for each population under the fitted modelâ€”handy for quick fit checks.


```{r}
# Huiâ€“Walter MLE (2 tests Ã— 2 populations), with SEs and Wald CIs
# pop1, pop2: 2x2 integer matrices (rows T1=0/1, cols T2=0/1)
hui_walter_mle_se <- function(pop1, pop2, conf_level = 0.95) {
  stopifnot(is.matrix(pop1), is.matrix(pop2),
            all(dim(pop1) == c(2,2)), all(dim(pop2) == c(2,2)))
  invlogit <- function(eta) 1/(1+exp(-eta))
  logit    <- function(p) log(p/(1-p))

  # Likelihood pieces
  Pcells <- function(se1, sp1, se2, sp2, pi) {
    c(
      p00 = pi*(1-se1)*(1-se2) + (1-pi)*sp1*sp2,
      p01 = pi*(1-se1)*se2     + (1-pi)*sp1*(1-sp2),
      p10 = pi*se1*(1-se2)     + (1-pi)*(1-sp1)*sp2,
      p11 = pi*se1*se2         + (1-pi)*(1-sp1)*(1-sp2)
    )
  }
  nll <- function(par) {
    se1 <- invlogit(par[1]); sp1 <- invlogit(par[2])
    se2 <- invlogit(par[3]); sp2 <- invlogit(par[4])
    pi1 <- invlogit(par[5]); pi2 <- invlogit(par[6])

    p1 <- pmax(Pcells(se1, sp1, se2, sp2, pi1), 1e-12)
    p2 <- pmax(Pcells(se1, sp1, se2, sp2, pi2), 1e-12)

    # counts in (00,01,10,11) order
    x1 <- c(pop1[1,1], pop1[1,2], pop1[2,1], pop1[2,2])
    x2 <- c(pop2[1,1], pop2[1,2], pop2[2,1], pop2[2,2])

    -(sum(x1*log(p1)) + sum(x2*log(p2)))
  }

  # crude starting values
  N1 <- sum(pop1); N2 <- sum(pop2)
  t1pos <- (pop1[2,1]+pop1[2,2] + pop2[2,1]+pop2[2,2])/(N1+N2)
  t2pos <- (pop1[1,2]+pop1[2,2] + pop2[1,2]+pop2[2,2])/(N1+N2)
  pclip <- function(p) pmax(0.05, pmin(0.95, p))
  par0 <- c(logit(pclip(t1pos+0.2)), logit(pclip(1-t1pos+0.2)),
            logit(pclip(t2pos+0.2)), logit(pclip(1-t2pos+0.2)),
            logit(0.3), logit(0.7))

  fit <- optim(par0, nll, method = "BFGS", hessian = TRUE, control = list(maxit = 1e4))
  if (fit$convergence != 0) warning("optim did not fully converge (code = ", fit$convergence, ").")

  # transform back
  est <- invlogit(fit$par)
  names(est) <- c("Se1","Sp1","Se2","Sp2","Prev_pop1","Prev_pop2")

  # variance-covariance for logits -> delta to probability scale
  # J = diag( p*(1-p) ) for invlogit componentwise
  gprime <- function(eta) { p <- invlogit(eta); p*(1-p) }
  J <- diag(gprime(fit$par), 6, 6)

  cov_eta <- try(solve(fit$hessian), silent = TRUE)
  if (inherits(cov_eta, "try-error")) {
    # fallback: numerical Hessian if needed
    if (!requireNamespace("numDeriv", quietly = TRUE))
      stop("numDeriv needed for numerical Hessian fallback. Install it or re-run.")
    cov_eta <- try(solve(numDeriv::hessian(nll, fit$par)), silent = TRUE)
  }

  if (inherits(cov_eta, "try-error")) {
    se <- rep(NA_real_, 6)
    cov_p <- matrix(NA_real_, 6, 6)
  } else {
    cov_p <- J %*% cov_eta %*% J
    se <- sqrt(pmax(diag(cov_p), 0))
  }
  names(se) <- names(est)

  # Wald CIs
  z <- qnorm(0.5 + conf_level/2)
  ci <- t(mapply(function(p, s) c(lower = max(0, p - z*s), upper = min(1, p + z*s)),
                 est, se))
  rownames(ci) <- names(est)

  # expected counts at MLE (useful diagnostics)
  ecounts <- function(pi) {
    p <- Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], pi)
    list(p = p,
         pop1 = round(p * N1, 2),
         pop2 = round(p * N2, 2))
  }

  list(
    estimates = est,
    se = se,
    ci = ci,
    cov_logit = cov_eta,
    cov_prob  = cov_p,
    logLik = -fit$value,
    convergence = fit$convergence,
    expected = list(
      pop1 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop1"]) * N1, 2),
      pop2 = round(Pcells(est["Se1"], est["Sp1"], est["Se2"], est["Sp2"], est["Prev_pop2"]) * N2, 2)
    )
  )
}

# --- Example ---
pop1 <- matrix(c(180, 20,
                  25, 75), nrow = 2, byrow = TRUE)
pop2 <- matrix(c(140, 10,
                  35, 115), nrow = 2, byrow = TRUE)

res <- hui_walter_mle_se(pop1, pop2)

res$estimates   # MLEs on probability scale
res$se          # standard errors (Wald)
res$ci          # Wald 95% CIs
res$logLik
```



**End of Document**
